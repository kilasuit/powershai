---
external help file: powershai-help.xml
schema: 2.0.0
powershai: true
---

# Get-AiChat

## SYNOPSIS <!--!= @#Synop !-->
Sends messages to an LLM and returns the response

## DESCRIPTION <!--!= @#Desc !-->
This is the most basic form of Chat promoted by PowershAI.  
With this function, you can send a message to an LLM from the current provider.  

This function is a lower level, standardized way to access an LLM that powershai provides.  
It does not manage history or context. It is useful for invoking simple prompts that do not require multiple interactions like a Chat.
Although it supports Function Calling, it does not execute any code and only returns the model's response.

** INFORMATION FOR PROVIDERS
	The provider must implement the Chat function for this functionality to be available. 
	The chat function should return an object with the response with the same specification as the OpenAI, Chat Completion function.
	The following links serve as a basis:
		https://platform.openai.com/docs/guides/chat-completions
		https://platform.openai.com/docs/api-reference/chat/object (return without streaming)
	The provider must implement the parameters of this function. 
	See the documentation for each parameter for details and how to map to a provider;
	
	When the model does not support one of the informed parameters (that is, there is no equivalent functionality, or that can be implemented in an equivalent way) an error should be returned.

## SYNTAX <!--!= @#Syntax !-->

```
Get-AiChat [[-prompt] <Object>] [[-temperature] <Object>] [[-model] <Object>] [[-MaxTokens] <Object>] [[-ResponseFormat] <Object>] [[-Functions] <Object>] [[-RawParams] 
<Object>] [[-StreamCallback] <Object>] [-IncludeRawResp] [<CommonParameters>]
```

## PARAMETERS <!--!= @#Params !-->

### -prompt
The prompt to be sent. It must be in the format described by the ConvertTo-OpenaiMessage function

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 1
Default Value: 
Accept pipeline input: false
Accept wildcard characters: false
```

### -temperature
Model temperature

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 2
Default Value: 0.6
Accept pipeline input: false
Accept wildcard characters: false
```

### -model
Model name. If not specified, it uses the provider's default.

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 3
Default Value: 
Accept pipeline input: false
Accept wildcard characters: false
```

### -MaxTokens
Maximum number of tokens to be returned

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 4
Default Value: 1024
Accept pipeline input: false
Accept wildcard characters: false
```

### -ResponseFormat
Response format
The acceptable formats, and behavior, should follow the same as OpenAI: https://platform.openai.com/docs/api-reference/chat/create#chat-create-response_format
Shortcuts:
	"json", is equivalent to {"type": "json_object"}

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 5
Default Value: 
Accept pipeline input: false
Accept wildcard characters: false
```

### -Functions
List of tools that should be invoked!
You can use commands like Get-OpenaiTool*, to easily transform powershell functions into the expected format!
If the model invokes the function, the response, both in stream, and normal, must also follow the OpenAI tool caling model.
This parameter must follow the same schema as OpenAI Function Calling: https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 6
Default Value: @()
Accept pipeline input: false
Accept wildcard characters: false
```

### -RawParams
Specify direct parameters of the provider's API.
This will overwrite the values that were calculated and generated based on the other parameters.

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 7
Default Value: @{}
Accept pipeline input: false
Accept wildcard characters: false
```

### -StreamCallback
Enables the Stream model
You must specify a ScriptBlock that will be invoked for each piece of text generated by the LLM.
The script should receive a parameter that represents each piece, in the same streaming format returned
	This parameter is an object that will contain the choices property, which is in the same schema returned by OpenAI streaming:
		https://platform.openai.com/docs/api-reference/chat/streaming

```yml
Parameter Set: (All)
Type: Object
Aliases: 
Accepted Values: 
Required: false
Position: 8
Default Value: 
Accept pipeline input: false
Accept wildcard characters: false
```

### -IncludeRawResp
Include the API response in a field called IncludeRawResp

```yml
Parameter Set: (All)
Type: SwitchParameter
Aliases: 
Accepted Values: 
Required: false
Position: named
Default Value: False
Accept pipeline input: false
Accept wildcard characters: false
```




<!--PowershaiAiDocBlockStart-->
_Automatically translated using PowershAI and AI_
<!--PowershaiAiDocBlockEnd-->
