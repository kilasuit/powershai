---
external help file: powershai-help.xml
Module Name: powershai
online version:
schema: 2.0.0
---

# Get-AiChat

## SYNOPSIS
Sends messages to a LLM and returns the response

## SYNTAX

```
Get-AiChat [[-prompt] <Object>] [[-temperature] <Object>] [[-model] <Object>] [[-MaxTokens] <Object>]
 [[-ResponseFormat] <Object>] [[-Functions] <Object>] [[-RawParams] <Object>] [[-StreamCallback] <Object>]
 [<CommonParameters>]
```

## DESCRIPTION
This is the most basic form of Chat promoted by PowershAI.
 
With this function, you can send a message to a LLM from the current provider.
 

This function is a lower-level, standardized way to access a LLM that powershai provides.
 
It does not manage history or context.
It is useful for invoking simple prompts that do not require multiple interactions like in a Chat. 
Although it supports Function Calling, it does not execute any code and only returns the model's response.



** PROVIDER INFORMATION
	The provider must implement the Chat function for this functionality to be available. 
	The chat function must return an object with the response following the same specification as OpenAI, Chat Completion function.
	The following links serve as a basis:
		https://platform.openai.com/docs/guides/chat-completions
		https://platform.openai.com/docs/api-reference/chat/object (non-streaming return)
	The provider must implement the parameters of this function. 
	See the documentation for each parameter for details and how to map to a provider;
	
	When the model does not support one of the specified parameters (i.e., there is no equivalent functionality, or it cannot be implemented in an equivalent manner), an error should be returned.

## EXAMPLES

### Example 1
```powershell
PS C:\> {{ Add example code here }}
```

{{ Add example description here }}

## PARAMETERS

### -prompt
The prompt to be sent.
Must be in the format described by the ConvertTo-OpenaiMessage function

```yaml
Type: Object
Parameter Sets: (All)
Aliases:

Required: False
Position: 1
Default value: None
Accept pipeline input: False
Accept wildcard characters: False
```

### -temperature
Model temperature

```yaml
Type: Object
Parameter Sets: (All)
Aliases:

Required: False
Position: 2
Default value: 0.6
Accept pipeline input: False
Accept wildcard characters: False
```

### -model
Model name.
If not specified, uses the default from the provider.

```yaml
Type: Object
Parameter Sets: (All)
Aliases:

Required: False
Position: 3
Default value: None
Accept pipeline input: False
Accept wildcard characters: False
```

### -MaxTokens
Maximum tokens to be returned

```yaml
Type: Object
Parameter Sets: (All)
Aliases:

Required: False
Position: 4
Default value: 1024
Accept pipeline input: False
Accept wildcard characters: False
```

### -ResponseFormat
Response format 
The acceptable formats and behavior should follow the same as OpenAI: https://platform.openai.com/docs/api-reference/chat/create#chat-create-response_format
Shortcuts:
	"json", equivalent to {"type": "json_object"}

```yaml
Type: Object
Parameter Sets: (All)
Aliases:

Required: False
Position: 5
Default value: None
Accept pipeline input: False
Accept wildcard characters: False
```

### -Functions
List of tools that should be invoked!
You can use commands like Get-OpenaiTool*, to easily transform PowerShell functions into the expected format!
If the model invokes the function, the response, both in stream and normal, should also follow the OpenAI tool calling model.
This parameter should follow the same scheme as OpenAI's Function Calling: https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools

```yaml
Type: Object
Parameter Sets: (All)
Aliases:

Required: False
Position: 6
Default value: @()
Accept pipeline input: False
Accept wildcard characters: False
```

### -RawParams
Specify direct parameters from the provider's API.
This will override the values that were calculated and generated based on the other parameters.

```yaml
Type: Object
Parameter Sets: (All)
Aliases:

Required: False
Position: 7
Default value: @{}
Accept pipeline input: False
Accept wildcard characters: False
```

### -StreamCallback
Enables the Stream model 
You must specify a ScriptBlock that will be invoked for each text generated by the LLM.
The script should receive a parameter representing each piece, in the same streaming format returned
	This parameter is an object that will contain the property choices, which follows the same scheme returned by OpenAI's streaming:
		https://platform.openai.com/docs/api-reference/chat/streaming

```yaml
Type: Object
Parameter Sets: (All)
Aliases:

Required: False
Position: 8
Default value: None
Accept pipeline input: False
Accept wildcard characters: False
```

### CommonParameters
This cmdlet supports the common parameters: -Debug, -ErrorAction, -ErrorVariable, -InformationAction, -InformationVariable, -OutVariable, -OutBuffer, -PipelineVariable, -Verbose, -WarningAction, and -WarningVariable. For more information, see [about_CommonParameters](http://go.microsoft.com/fwlink/?LinkID=113216).

## INPUTS

## OUTPUTS

## NOTES

## RELATED LINKS



_Automatically translated using PowershAI and AI._
